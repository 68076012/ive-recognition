{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IVE Face Recognition (InsightFace + Tracking)\n",
        "โปรเจ็กต์แฮกทอนสัปดาห์ที่ 5: ทำ Face Recognition + Tracking จากวิดีโอสัมภาษณ์ IVE (YouTube) โดยเน้นความนิ่งและความแม่นยำของชื่อที่ติดบนใบหน้า ใช้ชุดอ้างอิงที่เตรียมไว้ใน `project_recognition/ive_reference/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing: ['insightface==0.7.3', 'onnxruntime-gpu==1.17.1', 'opencv-python==4.8.1.78', 'scipy==1.10.1', 'filterpy==1.4.5']\n",
            "ORT providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "OpenCV version: 4.8.1 CUDA devices: 0\n"
          ]
        }
      ],
      "source": [
        "# Setup: install deps (GPU-first), skip if already installed\n",
        "import sys, subprocess, pkg_resources, os\n",
        "\n",
        "def pip_install(requirements):\n",
        "    to_install = []\n",
        "    for req in requirements:\n",
        "        try:\n",
        "            pkg_resources.require(req)\n",
        "        except pkg_resources.DistributionNotFound:\n",
        "            to_install.append(req)\n",
        "        except pkg_resources.VersionConflict:\n",
        "            to_install.append(req)\n",
        "    if to_install:\n",
        "        print(\"Installing:\", to_install)\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + to_install)\n",
        "    else:\n",
        "        print(\"All deps already present.\")\n",
        "\n",
        "requirements = [\n",
        "    \"insightface==0.7.3\",\n",
        "    \"onnxruntime-gpu==1.17.1\",\n",
        "    \"opencv-python==4.8.1.78\",\n",
        "    \"scipy==1.10.1\",\n",
        "    \"filterpy==1.4.5\",\n",
        "    \"tqdm>=4.66.1\",\n",
        "]\n",
        "pip_install(requirements)\n",
        "\n",
        "# Quick CUDA provider sanity check for onnxruntime\n",
        "try:\n",
        "    import onnxruntime as ort\n",
        "    print(\"ORT providers:\", ort.get_available_providers())\n",
        "except Exception as e:\n",
        "    print(\"ORT check failed:\", e)\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "    print(\"OpenCV version:\", cv2.__version__, \"CUDA devices:\", cv2.cuda.getCudaEnabledDeviceCount())\n",
        "except Exception as e:\n",
        "    print(\"OpenCV check failed:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reference dir: e:\\Master_Degree\\project_recognition\\ive_reference\n",
            "Output dir: e:\\Master_Degree\\project_recognition\\outputs\n"
          ]
        }
      ],
      "source": [
        "# Imports & basic setup\n",
        "import os, math, time, glob, json, random\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter, deque\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# InsightFace\n",
        "from insightface.app import FaceAnalysis\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Paths (try local notebook dir first, fallback to project root layout)\n",
        "ROOT = Path.cwd()\n",
        "if (ROOT / \"ive_reference\").exists():\n",
        "    REFERENCE_ROOT = ROOT / \"ive_reference\"\n",
        "    OUTPUT_DIR = ROOT / \"outputs\"\n",
        "else:\n",
        "    REFERENCE_ROOT = ROOT / \"week05\" / \"project_recognition\" / \"ive_reference\"\n",
        "    OUTPUT_DIR = ROOT / \"week05\" / \"project_recognition\" / \"outputs\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Video settings (use existing local file; no downloading)\n",
        "local_video_fallback = OUTPUT_DIR / \"ive_interview_input.mp4\"\n",
        "\n",
        "print(\"Reference dir:\", REFERENCE_ROOT)\n",
        "print(\"Output dir:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers: video utilities (local file only)\n",
        "\n",
        "def open_video(path: Path):\n",
        "    cap = cv2.VideoCapture(str(path))\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Cannot open video: {path}\")\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    return cap, fps, w, h, total\n",
        "\n",
        "\n",
        "def sample_frames(path: Path, num_samples: int = 3):\n",
        "    cap, fps, w, h, total = open_video(path)\n",
        "    indices = np.linspace(0, max(total - 1, 1), num=num_samples, dtype=int)\n",
        "    frames = []\n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
        "        ok, frame = cap.read()\n",
        "        if ok:\n",
        "            frames.append(frame)\n",
        "    cap.release()\n",
        "    print(f\"Sampled {len(frames)} frames of {total} | {w}x{h} @ {fps:.2f} fps\")\n",
        "    return frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "Using providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "*************** EP Error ***************\n",
            "EP Error D:\\a\\_work\\1\\s\\onnxruntime\\python\\onnxruntime_pybind_state.cc:857 onnxruntime::python::CreateExecutionProviderInstance CUDA_PATH is set but CUDA wasnt able to be loaded. Please install the correct version of CUDA andcuDNN as mentioned in the GPU requirements page  (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements),  make sure they're in the PATH, and that your GPU is supported.\n",
            " when using ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
            "****************************************\n",
            "[WARN] GPU init failed, fallback to CPU: D:\\a\\_work\\1\\s\\onnxruntime\\python\\onnxruntime_pybind_state.cc:857 onnxruntime::python::CreateExecutionProviderInstance CUDA_PATH is set but CUDA wasnt able to be loaded. Please install the correct version of CUDA andcuDNN as mentioned in the GPU requirements page  (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements),  make sure they're in the PATH, and that your GPU is supported.\n",
            "\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: C:\\Users\\feelc/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: C:\\Users\\feelc/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: C:\\Users\\feelc/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: C:\\Users\\feelc/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: C:\\Users\\feelc/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n",
            "Model keys: ['landmark_3d_68', 'landmark_2d_106', 'detection', 'genderage', 'recognition']\n",
            "Recognition providers: None\n"
          ]
        }
      ],
      "source": [
        "# Initialize InsightFace models (prefer GPU if available)\n",
        "\n",
        "def init_face_analysis():\n",
        "    try:\n",
        "        import onnxruntime as ort\n",
        "        available = ort.get_available_providers()\n",
        "    except Exception as e:\n",
        "        print(\"ORT provider check failed, falling back to CPU:\", e)\n",
        "        available = [\"CPUExecutionProvider\"]\n",
        "\n",
        "    preferred = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
        "    providers = [p for p in preferred if p in available]\n",
        "    if not providers:\n",
        "        providers = [\"CPUExecutionProvider\"]\n",
        "    print(\"Available providers:\", available)\n",
        "    print(\"Using providers:\", providers)\n",
        "\n",
        "    ctx_id = 0 if \"CUDAExecutionProvider\" in providers else -1\n",
        "    try:\n",
        "        app = FaceAnalysis(name=\"buffalo_l\", providers=providers)\n",
        "        app.prepare(ctx_id=ctx_id, det_size=(640, 640))\n",
        "    except RuntimeError as e:\n",
        "        print(\"[WARN] GPU init failed, fallback to CPU:\", e)\n",
        "        providers = [\"CPUExecutionProvider\"]\n",
        "        ctx_id = -1\n",
        "        app = FaceAnalysis(name=\"buffalo_l\", providers=providers)\n",
        "        app.prepare(ctx_id=ctx_id, det_size=(640, 640))\n",
        "\n",
        "    model_keys = list(app.models.keys())\n",
        "    recog_key = \"face_recognition\" if \"face_recognition\" in app.models else (\"recognition\" if \"recognition\" in app.models else None)\n",
        "    print(\"Model keys:\", model_keys)\n",
        "    if recog_key:\n",
        "        model_obj = app.models[recog_key]\n",
        "        prov = None\n",
        "        if hasattr(model_obj, \"sess\") and hasattr(model_obj.sess, \"providers\"):\n",
        "            prov = model_obj.sess.providers\n",
        "        elif hasattr(model_obj, \"providers\"):\n",
        "            prov = model_obj.providers\n",
        "        print(\"Recognition providers:\", prov)\n",
        "    else:\n",
        "        print(\"[WARN] recognition model key not found; available:\", model_keys)\n",
        "    return app, providers\n",
        "\n",
        "\n",
        "app, providers = init_face_analysis()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\feelc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gallery built: {'An Yujin': 29, 'Jang Wonyoung': 26, 'Kim Gaeul': 29, 'Kim Jiwon': 38, 'Lee Hyunseo': 39, 'Naoi Rei': 25}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(['An Yujin',\n",
              "  'Jang Wonyoung',\n",
              "  'Kim Gaeul',\n",
              "  'Kim Jiwon',\n",
              "  'Lee Hyunseo',\n",
              "  'Naoi Rei'],\n",
              " 6)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build gallery from reference images\n",
        "\n",
        "def normalize_embedding(emb):\n",
        "    norm = np.linalg.norm(emb) + 1e-9\n",
        "    return emb / norm\n",
        "\n",
        "\n",
        "def build_gallery(reference_root: Path, max_imgs_per_id: int = 50):\n",
        "    gallery = {}\n",
        "    stats = {}\n",
        "    for person_dir in sorted(reference_root.glob(\"*\")):\n",
        "        if not person_dir.is_dir():\n",
        "            continue\n",
        "        name = person_dir.name.replace(\"_\", \" \")\n",
        "        embeds = []\n",
        "        files = sorted(person_dir.glob(\"*\"))[:max_imgs_per_id]\n",
        "        for fp in files:\n",
        "            img = cv2.imread(str(fp))\n",
        "            if img is None:\n",
        "                continue\n",
        "            faces = app.get(img)\n",
        "            if not faces:\n",
        "                continue\n",
        "            # pick largest face\n",
        "            faces = sorted(faces, key=lambda f: (f.bbox[2]-f.bbox[0])*(f.bbox[3]-f.bbox[1]), reverse=True)\n",
        "            emb = faces[0].normed_embedding\n",
        "            embeds.append(emb)\n",
        "        if len(embeds) == 0:\n",
        "            print(f\"[WARN] No face detected for {name}\")\n",
        "            continue\n",
        "        embeds = np.stack(embeds, axis=0)\n",
        "        mean_emb = normalize_embedding(embeds.mean(axis=0))\n",
        "        gallery[name] = mean_emb\n",
        "        stats[name] = len(embeds)\n",
        "    print(\"Gallery built:\", stats)\n",
        "    return gallery\n",
        "\n",
        "\n",
        "gallery = build_gallery(REFERENCE_ROOT)\n",
        "list(gallery.keys()), len(gallery)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recognition + tracking utilities\n",
        "\n",
        "COLORS = {}\n",
        "\n",
        "def get_color(track_id: int):\n",
        "    if track_id not in COLORS:\n",
        "        np.random.seed(track_id + 123)\n",
        "        COLORS[track_id] = tuple(int(x) for x in np.random.randint(30, 230, size=3))\n",
        "    return COLORS[track_id]\n",
        "\n",
        "\n",
        "def cosine_sim(a, b):\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))\n",
        "\n",
        "\n",
        "def best_match(embedding, gallery, threshold=0.45):\n",
        "    best_name = \"Unknown\"\n",
        "    best_score = -1\n",
        "    for name, ref_emb in gallery.items():\n",
        "        score = cosine_sim(embedding, ref_emb)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_name = name\n",
        "    if best_score < threshold:\n",
        "        best_name = \"Unknown\"\n",
        "    return best_name, best_score\n",
        "\n",
        "\n",
        "def iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    inter = max(0, xB - xA) * max(0, yB - yA)\n",
        "    areaA = max(0, boxA[2]-boxA[0]) * max(0, boxA[3]-boxA[1])\n",
        "    areaB = max(0, boxB[2]-boxB[0]) * max(0, boxB[3]-boxB[1])\n",
        "    denom = areaA + areaB - inter + 1e-6\n",
        "    return inter / denom\n",
        "\n",
        "\n",
        "class Track:\n",
        "    def __init__(self, track_id, bbox, embedding, name, sim, bbox_momentum=0.6):\n",
        "        self.id = track_id\n",
        "        self.bbox = bbox.astype(float)\n",
        "        self.embedding = embedding\n",
        "        self.name_history = deque([name], maxlen=10)\n",
        "        self.sim_history = deque([sim], maxlen=10)\n",
        "        self.lost = 0\n",
        "        self.last_update = 0\n",
        "        self.bbox_momentum = bbox_momentum\n",
        "\n",
        "    def update(self, bbox, embedding, name, sim):\n",
        "        # smooth bbox to reduce jitter/floating boxes\n",
        "        self.bbox = self.bbox_momentum * bbox + (1 - self.bbox_momentum) * self.bbox\n",
        "        # exponential moving average of embedding for stability\n",
        "        self.embedding = normalize_embedding(0.7 * self.embedding + 0.3 * embedding)\n",
        "        self.name_history.append(name)\n",
        "        self.sim_history.append(sim)\n",
        "        self.lost = 0\n",
        "        self.last_update = 0\n",
        "\n",
        "    @property\n",
        "    def stable_name(self):\n",
        "        counts = Counter([n for n in self.name_history if n != \"Unknown\"])\n",
        "        if not counts:\n",
        "            return \"Unknown\"\n",
        "        return counts.most_common(1)[0][0]\n",
        "\n",
        "    @property\n",
        "    def stable_sim(self):\n",
        "        if not self.sim_history:\n",
        "            return 0\n",
        "        return float(np.mean(self.sim_history))\n",
        "\n",
        "\n",
        "class SimpleTracker:\n",
        "    def __init__(self, iou_thr=0.5, embed_thr=0.45, max_lost=5, sim_update_thr=0.42, bbox_momentum=0.6):\n",
        "        self.iou_thr = iou_thr\n",
        "        self.embed_thr = embed_thr\n",
        "        self.max_lost = max_lost\n",
        "        self.sim_update_thr = sim_update_thr\n",
        "        self.bbox_momentum = bbox_momentum\n",
        "        self.tracks = []\n",
        "        self.next_id = 0\n",
        "\n",
        "    def step(self, detections):\n",
        "        # detections: list of dict(bbox, embedding, name, sim)\n",
        "        assigned = set()\n",
        "        for det in detections:\n",
        "            # skip weak matches to avoid label flicker\n",
        "            if det['sim'] < self.sim_update_thr:\n",
        "                continue\n",
        "            best_score = -1\n",
        "            best_track = None\n",
        "            for trk in self.tracks:\n",
        "                iou_score = iou(det['bbox'], trk.bbox)\n",
        "                sim_score = cosine_sim(det['embedding'], trk.embedding)\n",
        "                score = iou_score + sim_score\n",
        "                if iou_score > self.iou_thr and sim_score > self.embed_thr and score > best_score:\n",
        "                    best_score = score\n",
        "                    best_track = trk\n",
        "            if best_track:\n",
        "                best_track.update(det['bbox'], det['embedding'], det['name'], det['sim'])\n",
        "                assigned.add(best_track.id)\n",
        "            else:\n",
        "                trk = Track(self.next_id, det['bbox'], det['embedding'], det['name'], det['sim'], bbox_momentum=self.bbox_momentum)\n",
        "                self.tracks.append(trk)\n",
        "                self.next_id += 1\n",
        "        # mark lost\n",
        "        alive = []\n",
        "        for trk in self.tracks:\n",
        "            if trk.id not in assigned:\n",
        "                trk.lost += 1\n",
        "            if trk.lost <= self.max_lost:\n",
        "                alive.append(trk)\n",
        "        self.tracks = alive\n",
        "        return self.tracks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drawing helpers\n",
        "\n",
        "def draw_label(frame, bbox, text, color, sim=0):\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "    label = f\"{text} ({sim:.2f})\"\n",
        "    (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "    cv2.rectangle(frame, (x1, y1 - th - 8), (x1 + tw + 4, y1), color, -1)\n",
        "    cv2.putText(frame, label, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main pipeline: detect -> recognize -> track -> render\n",
        "\n",
        "def process_video(\n",
        "    video_path: Path,\n",
        "    output_path: Path,\n",
        "    gallery: dict,\n",
        "    det_thresh: float = 0.50,\n",
        "    rec_thresh: float = 0.45,\n",
        "    tracker_iou: float = 0.50,\n",
        "    tracker_embed: float = 0.45,\n",
        "    max_frames: int = None,\n",
        "    warmup: int = 0,\n",
        "    max_lost: int = 5,\n",
        "    bbox_momentum: float = 0.65,\n",
        "):\n",
        "    tracker = SimpleTracker(\n",
        "        iou_thr=tracker_iou,\n",
        "        embed_thr=tracker_embed,\n",
        "        max_lost=max_lost,\n",
        "        sim_update_thr=rec_thresh,\n",
        "        bbox_momentum=bbox_momentum,\n",
        "    )\n",
        "    cap, fps, w, h, total = open_video(video_path)\n",
        "    total_iter = total if max_frames is None else min(total, max_frames)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    writer = cv2.VideoWriter(str(output_path), fourcc, fps, (w, h))\n",
        "\n",
        "    frame_idx = 0\n",
        "    name_counts = Counter()\n",
        "    start = time.time()\n",
        "\n",
        "    pbar = tqdm(total=total_iter, desc=\"Processing\", ncols=100)\n",
        "    while frame_idx < total_iter:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        faces = app.get(frame)\n",
        "        detections = []\n",
        "        for f in faces:\n",
        "            if f.det_score < det_thresh:\n",
        "                continue\n",
        "            bbox = f.bbox.astype(float)\n",
        "            emb = normalize_embedding(f.normed_embedding)\n",
        "            name, sim = best_match(emb, gallery, threshold=rec_thresh)\n",
        "            detections.append({\"bbox\": bbox, \"embedding\": emb, \"name\": name, \"sim\": sim})\n",
        "\n",
        "        tracks = tracker.step(detections)\n",
        "        for trk in tracks:\n",
        "            color = get_color(trk.id)\n",
        "            label = trk.stable_name\n",
        "            name_counts[label] += 1\n",
        "            draw_label(frame, trk.bbox, f\"{label}#{trk.id}\", color, trk.stable_sim)\n",
        "\n",
        "        writer.write(frame)\n",
        "        frame_idx += 1\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    elapsed = time.time() - start\n",
        "    fps_run = frame_idx / max(elapsed, 1e-3)\n",
        "    print(f\"Done. Frames: {frame_idx}/{total_iter}, runtime: {elapsed:.1f}s, avg fps: {fps_run:.2f}\")\n",
        "    print(\"Name counts (rough hit frequency):\", name_counts)\n",
        "    return name_counts, fps_run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using video: e:\\Master_Degree\\project_recognition\\outputs\\ive_interview_input.mp4\n",
            "Sampled 1 frames of 13895 | 1280x720 @ 23.98 fps\n"
          ]
        }
      ],
      "source": [
        "# Resolve video path (local file only)\n",
        "custom_video_path = None  # e.g., Path(r\"E:/Downloads/ive_interview.mp4\")\n",
        "\n",
        "if custom_video_path and Path(custom_video_path).exists():\n",
        "    video_path = Path(custom_video_path)\n",
        "else:\n",
        "    video_path = local_video_fallback\n",
        "\n",
        "if not video_path.exists():\n",
        "    raise FileNotFoundError(f\"Video not found at {video_path}. Set custom_video_path to your MP4.\")\n",
        "\n",
        "print(\"Using video:\", video_path)\n",
        "_ = sample_frames(video_path, num_samples=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|█████████████████████████████████████████████████| 200/200 [01:32<00:00,  2.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done. Frames: 200/200, runtime: 92.6s, avg fps: 2.16\n",
            "Name counts (rough hit frequency): Counter({'An Yujin': 193, 'Kim Gaeul': 193, 'Kim Jiwon': 193, 'Lee Hyunseo': 192, 'Jang Wonyoung': 187, 'Naoi Rei': 184})\n",
            "Quick sample saved: e:\\Master_Degree\\project_recognition\\outputs\\ive_quickcheck.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Quick dry-run on first N frames for sanity (adjust N as needed)\n",
        "quick_output = OUTPUT_DIR / \"ive_quickcheck.mp4\"\n",
        "_ = process_video(\n",
        "    video_path=video_path,\n",
        "    output_path=quick_output,\n",
        "    gallery=gallery,\n",
        "    det_thresh=0.45,\n",
        "    rec_thresh=0.40,\n",
        "    tracker_iou=0.45,\n",
        "    tracker_embed=0.36,\n",
        "    max_frames=200,\n",
        ")\n",
        "print(\"Quick sample saved:\", quick_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████████████████████████████████████▉| 13893/13895 [1:30:09<00:00,  2.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done. Frames: 13893/13895, runtime: 5409.6s, avg fps: 2.57\n",
            "Name counts (rough hit frequency): Counter({'An Yujin': 11246, 'Lee Hyunseo': 9387, 'Kim Jiwon': 9048, 'Kim Gaeul': 9039, 'Jang Wonyoung': 8512, 'Naoi Rei': 8453})\n",
            "Saved: e:\\Master_Degree\\project_recognition\\outputs\\ive_recognized.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Full run on the whole video (may take several minutes)\n",
        "output_video = OUTPUT_DIR / \"ive_recognized.mp4\"\n",
        "name_counts, fps_run = process_video(\n",
        "    video_path=video_path,\n",
        "    output_path=output_video,\n",
        "    gallery=gallery,\n",
        "    det_thresh=0.45,\n",
        "    rec_thresh=0.40,\n",
        "    tracker_iou=0.45,\n",
        "    tracker_embed=0.36,\n",
        "    max_frames=None,  # set small int for quick dry run\n",
        ")\n",
        "print(\"Saved:\", output_video)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
